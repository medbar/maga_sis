{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT clf tutorial",
      "provenance": [],
      "collapsed_sections": [
        "_sna0XHVjE4n",
        "G4-kSSS87sK-",
        "RexUReS5jG3U",
        "pEcJVm_u8bQU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/medbar/maga_sis/blob/main/2/NLP/BERT_clf_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4ASjj5L9THg"
      },
      "source": [
        "* Полезно: [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
        "\n",
        "* Если модель есть в репозитории Huggingface, её можно загрузить и проинициализировать по названию.\n",
        "Например:\n",
        "https://huggingface.co/DeepPavlov/rubert-base-cased\n",
        "```\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rubert-base-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"rubert-base-cased\", num_labels=2)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_-bSfOchlpB"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY39sTGWfJkm"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", \n",
        "                                                           num_labels=2)  # Число классов в целевом датасете"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtaHRMw2jAp6"
      },
      "source": [
        "# Prepare data\n",
        "[Loading a dataset](https://huggingface.co/docs/datasets/quicktour.html#loading-a-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-CAUfNGoMN6"
      },
      "source": [
        "!pip install datasets  # тоже библиотека Huggingface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOXesI1IpcWg"
      },
      "source": [
        "### Сгенерируем элементарный датасет\n",
        "\n",
        "import random\n",
        "\n",
        "data = {0: [], 1: []}  # положительный и отрицательный класс\n",
        "\n",
        "for i in range(1000):\n",
        "    label = random.randint(0, 1)\n",
        "    if label == 1:\n",
        "        text = ['spam']\n",
        "    else:\n",
        "        text = ['ham']\n",
        "    text *= random.randint(10, 1000)\n",
        "    text = \" \".join(text)\n",
        "\n",
        "    data[label].append(text)\n",
        "\n",
        "for value in data.values():  # перемешаем каждый класс перед разбиением\n",
        "    random.shuffle(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR2hbTjAscgb"
      },
      "source": [
        "# Выберем пропорции для разбиения на train, eval и test сабсеты\n",
        "\n",
        "split_sizes = (0.8, 0.1, 0.1)\n",
        "assert sum(split_sizes) == 1\n",
        "\n",
        "# Возьмём нужную долю от каждого класса \n",
        "# (как sklearn.model_selection.train_test_split с арг. stratify)\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "data_split = {key: {} for key in [\"train\", \"eval\", \"test\"]}\n",
        "for no, partition in enumerate([\"train\", \"eval\", \"test\"]):\n",
        "    for label, samples in data.items():\n",
        "        sample_count = len(samples)\n",
        "\n",
        "        start = ceil(sum(split_sizes[:no]) * sample_count)\n",
        "        stop = ceil(sum(split_sizes[:no+1]) * sample_count)\n",
        "\n",
        "        data_split[partition][label] = samples[start:stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG2BNvufrqUU"
      },
      "source": [
        "### Запишем в файлы\n",
        "\n",
        "import csv\n",
        "\n",
        "for partition in [\"train\", \"eval\", \"test\"]:\n",
        "    with open(f\"dummy_data_{partition}.csv\", \"w+\") as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        writer.writerow(['text', 'label'])\n",
        "        for label, texts in data_split[partition].items():\n",
        "            for text in texts:\n",
        "                writer.writerow([text, label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKj9jq88o5Wr"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('csv', data_files={\n",
        "    'train': 'dummy_data_train.csv',\n",
        "    'eval': 'dummy_data_eval.csv',\n",
        "    'test': 'dummy_data_test.csv'\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TUibHGBjy-I"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPF-zu9enlaF"
      },
      "source": [
        "tokenized_dataset[\"train\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sna0XHVjE4n"
      },
      "source": [
        "# Train\n",
        "[Trainer](https://huggingface.co/transformers/main_classes/trainer.html)\n",
        "\n",
        "[TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZFZQMqCjEPV"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "# Аргументы дефолтные, feel free to experiment\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    num_train_epochs=100,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=.0,\n",
        "    save_total_limit=20,\n",
        "    output_dir='checkpoints',\n",
        "    logging_dir='logs',\n",
        "    # Для сохранения и валидации на каждом n-ном шаге — иначе сохраняется после каждой эпохи\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # save_strategy=\"steps\",\n",
        "    # eval_steps=50,\n",
        "    # save_steps=50,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzCUg6ne3mtv"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric_names = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "metrics = {name: load_metric(name) for name in metric_names}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {name: metric.compute(predictions=predictions, references=labels) for name, metric in metrics.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60M9gYum1oet"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    args=training_args, \n",
        "    train_dataset=tokenized_dataset[\"train\"], \n",
        "    eval_dataset=tokenized_dataset[\"eval\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnoZ8fcA2dvD"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4-kSSS87sK-"
      },
      "source": [
        "# Tensorboard\n",
        "Помогает наблюдать за обучением (запускать предварительно). Обычно запускается локально на некотором порте, открывается в браузере; но Колаб позволяет запустить его прямо в ячейке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1PPhNkO7rvx"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF0A_sjp7uXc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArOdiIxb7y7_"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RexUReS5jG3U"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FMHgeFF2pOz"
      },
      "source": [
        "trainer.evaluate(tokenized_dataset[\"test\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEcJVm_u8bQU"
      },
      "source": [
        "# Infer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-szlWmH19Fi_"
      },
      "source": [
        "import torch\n",
        "\n",
        "def infer(texts):\n",
        "    with torch.no_grad():\n",
        "        tokenized_texts = tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        tokenized_texts = tokenized_texts.to('cuda')\n",
        "        return model.forward(**tokenized_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPYAKcdJ-R4x"
      },
      "source": [
        "predictions = infer([\n",
        "       \"spam spam spam\",\n",
        "       \"ham ham ham ham\"\n",
        "])['logits'].cpu().numpy()\n",
        "\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}