{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f96cbf-2994-4f30-bd6d-4ac64cc77b6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import catboost\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from torch.utils.data import DataLoader\n",
    "from razdel import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90a0018-30bc-470f-9b5a-c798ee318e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_xml = ET.parse('data/jokes1.xml')\n",
    "# root = data_xml.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f12872-2ba9-49c6-9f11-01da63256e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sentence in enumerate(list(root)):\n",
    "#     item_seq = []\n",
    "#     for word in list(sentence.findall('word')):\n",
    "#         print(word.tag, word.attrib, word.text)\n",
    "#         for c in list(word):\n",
    "#             print(c.tag, c.attrib, c.text)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38ae93bf-3138-4e4e-84c0-b420eac95c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': 'Друзья',\n",
       "  'ph_trans': ['д', 'р', 'у', \"з'\", 'й', 'а'],\n",
       "  'allo_trans': ['d', 'r', 'u1', \"z'\", 'j', 'a0']},\n",
       " {'original': 'мои,',\n",
       "  'ph_trans': ['м', 'а', 'и'],\n",
       "  'allo_trans': ['m', 'a1', 'i0']},\n",
       " {'original': 'чтобы',\n",
       "  'ph_trans': ['ш', 'т', 'о', 'б', 'ы'],\n",
       "  'allo_trans': ['sh', 't', 'o0', 'b', 'y4']},\n",
       " {'original': 'соответствовать',\n",
       "  'ph_trans': ['с',\n",
       "   'а',\n",
       "   'а',\n",
       "   'т',\n",
       "   \"в'\",\n",
       "   'е',\n",
       "   'ц',\n",
       "   'т',\n",
       "   'в',\n",
       "   'а',\n",
       "   'в',\n",
       "   'а',\n",
       "   \"т'\"],\n",
       "  'allo_trans': ['s',\n",
       "   'a2',\n",
       "   'a1',\n",
       "   't',\n",
       "   \"v'\",\n",
       "   'e0',\n",
       "   'c',\n",
       "   't',\n",
       "   'v',\n",
       "   'a4',\n",
       "   'v',\n",
       "   'a4',\n",
       "   \"t'\"]},\n",
       " {'original': 'вам,',\n",
       "  'ph_trans': ['в', 'а', 'м'],\n",
       "  'allo_trans': ['v', 'a0', 'm']},\n",
       " {'original': 'я', 'ph_trans': ['й', 'а'], 'allo_trans': ['j', 'a0']},\n",
       " {'original': 'готов',\n",
       "  'ph_trans': ['г', 'а', 'т', 'о', 'в'],\n",
       "  'allo_trans': ['g', 'a1', 't', 'o0', 'v']},\n",
       " {'original': 'сделать',\n",
       "  'ph_trans': ['з', \"д'\", 'е', 'л', 'а', \"т'\"],\n",
       "  'allo_trans': ['z', \"d'\", 'e0', 'l', 'a4', \"t'\"]},\n",
       " {'original': 'над',\n",
       "  'ph_trans': ['н', 'а', 'т'],\n",
       "  'allo_trans': ['n', 'a2', 't']},\n",
       " {'original': 'собой',\n",
       "  'ph_trans': ['с', 'а', 'б', 'о', 'й'],\n",
       "  'allo_trans': ['s', 'a1', 'b', 'o0', 'j']},\n",
       " {'original': 'усилие',\n",
       "  'ph_trans': ['у', \"с'\", 'и', \"л'\", 'и', 'й', 'и'],\n",
       "  'allo_trans': ['u1', \"s'\", 'i0', \"l'\", 'i4', 'j', 'i4']},\n",
       " {'original': 'и', 'ph_trans': ['и'], 'allo_trans': ['i1']},\n",
       " {'original': 'стать',\n",
       "  'ph_trans': ['с', 'т', 'а', \"т'\"],\n",
       "  'allo_trans': ['s', 't', 'a0', \"t'\"]},\n",
       " {'original': 'лучше.',\n",
       "  'ph_trans': ['л', 'у', 'ч', 'ш', 'ы'],\n",
       "  'allo_trans': ['l', 'u0', 'ch', 'sh', 'y4']}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_base(path='data/jokes1.xml', count=float('inf')):   \n",
    "    data_xml = ET.parse(path)\n",
    "    root = data_xml.getroot()\n",
    "    sentences = []\n",
    "    for i, sent in enumerate(list(root)):\n",
    "        item_seq = []\n",
    "        for word in list(sent.findall('word')):\n",
    "            #print(word.tag, word.attrib, word.text)\n",
    "            if 'original' not in word.attrib:\n",
    "                continue\n",
    "            trans = [c.attrib['ph'] for c in list(word.findall('phoneme'))]\n",
    "            allo_trans = [c.attrib['ph'] for c in list(word.findall('allophone'))]\n",
    "            assert len(trans) == len(allo_trans), 'f{trans=} {allo_trans=}'\n",
    "            item = {'original': word.attrib['original'],\n",
    "                    'ph_trans': trans,\n",
    "                   'allo_trans': allo_trans}\n",
    "            item_seq.append(item)\n",
    "        sentences.append(item_seq)\n",
    "        if i >=count:\n",
    "            break\n",
    "    return sentences\n",
    "data = get_base()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b6b414-7754-41cd-b12c-24af37f006e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def char_phone_allo_conts(sents):\n",
    "    \n",
    "    chars = defaultdict(int)\n",
    "    phones = defaultdict(int)\n",
    "    allophones = defaultdict(int)\n",
    "    \n",
    "    for sent in sents:\n",
    "        for s in sent:\n",
    "            for c in s['original']:\n",
    "                chars[c]+=1\n",
    "            for p in s['ph_trans']:\n",
    "                phones[p]+=1\n",
    "            for a in s['allo_trans']:\n",
    "                allophones[a]+=1\n",
    "    return chars, phones, allophones\n",
    "\n",
    "chars_counts, phone_counts, allo_counts = char_phone_allo_conts(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c23865e-1f76-4917-90b1-facd9b642cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'д': 50602, 'р': 71950, 'у': 49895, 'з': 25565, 'ь': 31572, 'я': 31797, 'м': 50957, 'о': 167438, 'и': 106642, ',': 29352, 'ч': 26712, 'т': 110173, 'б': 27718, 'ы': 28949, 'с': 80270, 'в': 67144, 'е': 134906, 'а': 137005, 'г': 25304, 'л': 63432, 'н': 100070, 'й': 18369, 'ш': 14545, '.': 30688, 'х': 13062, 'ж': 17581, '!': 5156, '-': 21246, ' ': 8154, 'к': 59541, 'п': 46118, '?': 7062, 'ю': 11406, 'щ': 5281, ':': 4139, 'э': 4925, 'ц': 6493, 'ф': 3571, '1': 566, '0': 909, '\"': 3543, '3': 246, '$': 20, '8': 152, '2': 578, 'ъ': 546, '9': 139, '%': 110, '5': 333, '4': 185, 'w': 46, 'i': 144, 'n': 81, 'd': 71, 'o': 135, 's': 88, 'k': 37, 'e': 144, 'a': 115, 't': 82, '7': 116, '6': 123, 'p': 64, 'l': 73, 'c': 64, 'h': 49, 'q': 16, 'u': 44, 'f': 35, 'r': 81, 'y': 38, 'z': 8, 'b': 52, 'v': 24, 'g': 41, 'j': 3, 'm': 42, 'x': 24, '№': 8, '/': 3, '+': 7, 'ё': 4, 'ο': 3, '的': 1, '长': 1, '老': 1, '#': 3, '̆': 4, '́': 1}) defaultdict(<class 'int'>, {'д': 31316, 'р': 49742, 'у': 61152, \"з'\": 4426, 'й': 61850, 'а': 275812, 'м': 37974, 'и': 177084, 'ш': 21892, 'т': 74116, 'о': 64770, 'б': 19062, 'ы': 50186, 'с': 49638, \"в'\": 12822, 'е': 46401, 'ц': 12437, 'в': 46589, \"т'\": 29989, 'г': 17411, 'з': 21369, \"д'\": 19547, 'л': 30099, 'н': 58937, \"с'\": 21801, \"л'\": 32384, 'ч': 19088, \"н'\": 37981, 'х': 12769, 'ж': 16562, 'к': 50992, \"п'\": 8008, 'п': 39233, \"м'\": 12686, \"р'\": 22092, 'щ': 6364, \"б'\": 7313, 'ф': 15452, \"г'\": 2310, \"к'\": 7893, \"х'\": 553, \"ф'\": 1583}) defaultdict(<class 'int'>, {'d': 31316, 'r': 49742, 'u1': 20491, \"z'\": 4426, 'j': 61850, 'a0': 75381, 'm': 37974, 'a1': 88574, 'i0': 29945, 'sh': 21892, 't': 74116, 'o0': 62713, 'b': 19062, 'y4': 24524, 's': 49638, 'a2': 32238, \"v'\": 12822, 'e0': 46389, 'c': 12370, 'v': 46589, 'a4': 79619, \"t'\": 29989, 'g': 17411, 'z': 21369, \"d'\": 19547, 'l': 30099, 'n': 58937, \"s'\": 21801, \"l'\": 32384, 'i4': 75280, 'i1': 71859, 'u0': 20831, 'ch': 19026, 'o1': 1936, 'y0': 14694, \"n'\": 37981, 'h': 12031, 'zh': 16562, 'k': 50992, \"p'\": 8008, 'p': 39233, \"m'\": 12686, 'u4': 19830, \"r'\": 22092, 'sc': 6353, 'y1': 10968, \"b'\": 7313, 'f': 15452, \"g'\": 2310, \"k'\": 7893, 'H': 738, \"h'\": 553, \"f'\": 1583, 'C': 67, 'o4': 121, 'e1': 12, 'CH': 62, 'SC': 11})\n"
     ]
    }
   ],
   "source": [
    "print(chars_counts, phone_counts, allo_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f36af66-c1ed-46b5-96fe-122214161826",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_chars = {c:i for i,c in enumerate(['<PAD>', '<SOW>', '<EOW>', '<UNK>'] + list('абвгдеёжзийклмнопрстуфхцчшщъыьэюя,.!?-:'))}\n",
    "vocab_phones = {c:i for i, c in enumerate(['<PAD>', '<SOW>', '<EOW>'] + list(phone_counts.keys()))}\n",
    "vocab_allos = {c:i for i, c in enumerate(['<PAD>', '<SOW>', '<EOW>'] + list(allo_counts.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b865b2-c112-4bc8-8353-649d02ed0717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOW>': 1, '<EOW>': 2, '<UNK>': 3, 'а': 4, 'б': 5, 'в': 6, 'г': 7, 'д': 8, 'е': 9, 'ё': 10, 'ж': 11, 'з': 12, 'и': 13, 'й': 14, 'к': 15, 'л': 16, 'м': 17, 'н': 18, 'о': 19, 'п': 20, 'р': 21, 'с': 22, 'т': 23, 'у': 24, 'ф': 25, 'х': 26, 'ц': 27, 'ч': 28, 'ш': 29, 'щ': 30, 'ъ': 31, 'ы': 32, 'ь': 33, 'э': 34, 'ю': 35, 'я': 36, ',': 37, '.': 38, '!': 39, '?': 40, '-': 41, ':': 42} {'<PAD>': 0, '<SOW>': 1, '<EOW>': 2, 'д': 3, 'р': 4, 'у': 5, \"з'\": 6, 'й': 7, 'а': 8, 'м': 9, 'и': 10, 'ш': 11, 'т': 12, 'о': 13, 'б': 14, 'ы': 15, 'с': 16, \"в'\": 17, 'е': 18, 'ц': 19, 'в': 20, \"т'\": 21, 'г': 22, 'з': 23, \"д'\": 24, 'л': 25, 'н': 26, \"с'\": 27, \"л'\": 28, 'ч': 29, \"н'\": 30, 'х': 31, 'ж': 32, 'к': 33, \"п'\": 34, 'п': 35, \"м'\": 36, \"р'\": 37, 'щ': 38, \"б'\": 39, 'ф': 40, \"г'\": 41, \"к'\": 42, \"х'\": 43, \"ф'\": 44} {'<PAD>': 0, '<SOW>': 1, '<EOW>': 2, 'd': 3, 'r': 4, 'u1': 5, \"z'\": 6, 'j': 7, 'a0': 8, 'm': 9, 'a1': 10, 'i0': 11, 'sh': 12, 't': 13, 'o0': 14, 'b': 15, 'y4': 16, 's': 17, 'a2': 18, \"v'\": 19, 'e0': 20, 'c': 21, 'v': 22, 'a4': 23, \"t'\": 24, 'g': 25, 'z': 26, \"d'\": 27, 'l': 28, 'n': 29, \"s'\": 30, \"l'\": 31, 'i4': 32, 'i1': 33, 'u0': 34, 'ch': 35, 'o1': 36, 'y0': 37, \"n'\": 38, 'h': 39, 'zh': 40, 'k': 41, \"p'\": 42, 'p': 43, \"m'\": 44, 'u4': 45, \"r'\": 46, 'sc': 47, 'y1': 48, \"b'\": 49, 'f': 50, \"g'\": 51, \"k'\": 52, 'H': 53, \"h'\": 54, \"f'\": 55, 'C': 56, 'o4': 57, 'e1': 58, 'CH': 59, 'SC': 60}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_chars, vocab_phones, vocab_allos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d407936-165f-47e0-bc9f-0f7e9cb42e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab_chars, vocab_phones, vocab_allos):\n",
    "        self.vocab_chars = vocab_chars\n",
    "        self.vocab_phones = vocab_phones\n",
    "        self.vocab_allos = vocab_allos\n",
    "        self.to_str_dict = {'chars':  [None] * (max(self.vocab_chars.values())+1), \n",
    "            'phones': [None] * (max(self.vocab_phones.values())+1),\n",
    "            'allos': [None] * (max(self.vocab_allos.values())+1)}\n",
    "        \n",
    "        for p, p_id in self.vocab_chars.items():\n",
    "            self.to_str_dict['chars'][p_id]=p\n",
    "        \n",
    "        for p, p_id in self.vocab_phones.items():\n",
    "            self.to_str_dict['phones'][p_id]=p\n",
    "        \n",
    "        for p, p_id in self.vocab_allos.items():\n",
    "            self.to_str_dict['allos'][p_id]=p\n",
    "                                   \n",
    "    \n",
    "    def get_num_chars(self):\n",
    "        return max(self.vocab_chars.values())\n",
    "    \n",
    "    def get_num_phones(self):\n",
    "        return max(self.vocab_phones.values())\n",
    "        \n",
    "    def get_num_allos(self):\n",
    "        return len(self.vocab_allos)\n",
    "    \n",
    "    def get_char_pad(self):\n",
    "        return self.vocab_chars['<PAD>']\n",
    "    \n",
    "    def get_phone_pad(self):\n",
    "        return self.vocab_phones['<PAD>']\n",
    "    \n",
    "    def get_allo_pad(self):\n",
    "        return self.vocab_allos['<PAD>']\n",
    "            \n",
    "    def get_unk(self):\n",
    "        return self.vocab_chars['<UNK>']\n",
    "    \n",
    "    def get_char_eos(self):\n",
    "        return self.vocab_phones['<EOW>']\n",
    "    \n",
    "    def get_phone_eos(self):\n",
    "        return self.vocab_phones['<EOW>']\n",
    "    \n",
    "    def get_allo_eos(self):\n",
    "        return self.vocab_allos['<EOW>']\n",
    "    \n",
    "    def get_char_sos(self):\n",
    "        return self.vocab_phones['<SOW>']\n",
    "    \n",
    "    def get_phone_sos(self):\n",
    "        return self.vocab_phones['<SOW>']\n",
    "    \n",
    "    def get_allo_sos(self):\n",
    "        return self.vocab_allos['<SOW>']\n",
    "    \n",
    "    def tokenize_chars(self, chars, eos=True, sos=True):\n",
    "        chars = [self.vocab_chars[c] if c in self.vocab_chars else self.get_unk() for c in chars]\n",
    "        if eos:\n",
    "            chars = chars + [self.get_char_eos()]\n",
    "        if sos:\n",
    "            chars =  [self.get_char_sos()] + chars \n",
    "        return  torch.LongTensor(chars)\n",
    "    \n",
    "    def tokenize_item(self, item, eos=True, sos=True):\n",
    "        phones = [self.vocab_phones[p] for p in item['ph_trans']]\n",
    "        allos = [self.vocab_allos[p] for p in item['allo_trans']]\n",
    "        if eos:\n",
    "            phones = phones + [self.get_phone_eos()]\n",
    "            allos = allos + [self.get_allo_eos()]\n",
    "        if sos:\n",
    "            phones = [self.get_phone_sos()] + phones\n",
    "            allos = [self.get_allo_sos()] + allos \n",
    "        \n",
    "        return {'original': item['original'],\n",
    "                'chars': self.tokenize_chars(item['original']),\n",
    "                'phones': torch.LongTensor(phones),\n",
    "                'allos': torch.LongTensor(allos)}\n",
    "    \n",
    "    def convert_to_str(self, ids, ids_type='phones'):\n",
    "        return [self.to_str_dict[ids_type][p_id] for p_id in ids]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aedb42ed-7f62-4314-8db9-d3efcd2a465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_chars, vocab_phones, vocab_allos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af6eabaa-a316-4735-8b9e-7c2c908773ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_sents)=28722 len(test_sents)=3192\n"
     ]
    }
   ],
   "source": [
    "train_sents, test_sents = train_test_split(data, random_state=42, train_size=0.9)\n",
    "print(f'{len(train_sents)=} {len(test_sents)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53ecec2-1ad5-48a9-8d14-211e2ca761f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'собираетесь',\n",
       " 'chars': tensor([ 1, 22, 19,  5, 13, 21,  4,  9, 23,  9, 22, 33,  2]),\n",
       " 'phones': tensor([ 1, 16,  8, 39, 10,  4,  8,  7, 10, 21, 10, 27,  2]),\n",
       " 'allos': tensor([ 1, 17, 18, 49, 33,  4,  8,  7, 32, 24, 32, 30,  2])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize_item(train_sents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c66a1ff4-f9ba-47cb-8e83-7c9d8e429a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqTranscriptorDataSet:\n",
    "    def __init__(self, sents, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized = [[tokenizer.tokenize_item(item) for item in s] for s in sents]\n",
    "        self.indexes = [(s_id, w_id) for s_id, s in enumerate(self.tokenized) for w_id in range(len(s))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "    \n",
    "    def size(self, index):\n",
    "        s_id, w_id = self.indexes[index]\n",
    "        return len(self.tokenized[s_id][w_id])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        s_id, w_id = self.indexes[index]\n",
    "        return self.tokenized[s_id][w_id]\n",
    "    \n",
    "    def collate(self, items):\n",
    "        chars = [item['chars'] for item in items]\n",
    "        phones = [item['phones'] for item in items]\n",
    "        allos = [item['allos'] for item in items]\n",
    "        orig = [item['original'] for item in items]\n",
    "        #print(items)\n",
    "        return {'chars': pad_sequence(chars, batch_first=False, padding_value=self.tokenizer.get_char_pad()), #sl X btz\n",
    "                'phones': pad_sequence(phones, batch_first=False, padding_value=self.tokenizer.get_phone_pad()),\n",
    "                'allos': pad_sequence(allos, batch_first=False, padding_value=self.tokenizer.get_allo_pad()),\n",
    "                'original': orig}\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7813c52a-e3df-48c3-9352-03184282f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqTranscriptorDataSet:\n",
    "    def __init__(self, sents, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        tokenized = [tokenizer.tokenize_item(item) for s in sents for item in s]\n",
    "        print(f'Total words {len(tokenized)}')\n",
    "        tokenized = {item['original']: item \\\n",
    "                          for item in tokenized}\n",
    "        print(f'Total uniq words {len(tokenized)}')\n",
    "        self.tokenized = [list(tokenized.values())]\n",
    "        self.indexes = [(s_id, w_id) for s_id, s in enumerate(self.tokenized) for w_id in range(len(s))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "    \n",
    "    def size(self, index):\n",
    "        s_id, w_id = self.indexes[index]\n",
    "        item = self.tokenized[s_id][w_id]\n",
    "        return len(item['original']), len(item['phones'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        s_id, w_id = self.indexes[index]\n",
    "        return self.tokenized[s_id][w_id]\n",
    "    \n",
    "    def collate(self, items):\n",
    "        chars = [item['chars'] for item in items]\n",
    "        phones = [item['phones'] for item in items]\n",
    "        allos = [item['allos'] for item in items]\n",
    "        orig = [item['original'] for item in items]\n",
    "        #print(items)\n",
    "        return {'chars': pad_sequence(chars, batch_first=False, padding_value=self.tokenizer.get_char_pad()), #sl X btz\n",
    "                'phones': pad_sequence(phones, batch_first=False, padding_value=self.tokenizer.get_phone_pad()),\n",
    "                'allos': pad_sequence(allos, batch_first=False, padding_value=self.tokenizer.get_allo_pad()),\n",
    "                'original': orig}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3ce56fb-45e8-4d8c-b046-ef63b0e295c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SortedSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        self.sizes_and_index = [(self.ds.size(i), i) for i in range(len(self.ds))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sizes_and_index)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((i for s, i in sorted(self.sizes_and_index)))\n",
    "    \n",
    "def make_sorted_dataloader(ds, min_sample_len=2, **kwargs):\n",
    "    assert 'collate_fn' not in kwargs and 'sampler' not in kwargs and 'shuffle' not in kwargs , f\"bad kwargs {kwargs}\"\n",
    "    return DataLoader(ds, \n",
    "                      collate_fn=ds.collate, \n",
    "                      sampler=SortedSampler(ds),\n",
    "                      shuffle=False, \n",
    "                      **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5e90761-6dd5-49cd-9b9d-0d2b4c944793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "class TranscriptTransformer(nn.Module):\n",
    "    def __init__(self, input_vs, out_vs, dim=512, nhead=8, \n",
    "                                                num_encoder_layers=6, \n",
    "                                                num_decoder_layers=6, \n",
    "                                                dim_feedforward=1024, \n",
    "                                                dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim= dim\n",
    "        self.embeddings = nn.Embedding(input_vs, dim, padding_idx=0)\n",
    "        self.pos_embs = PositionalEncoding(dim, 0, 40)\n",
    "        self.out_embs = nn.Embedding(out_vs, dim, padding_idx=0)\n",
    "        self.transformer = torch.nn.Transformer(d_model=dim, \n",
    "                                                nhead=nhead, \n",
    "                                                num_encoder_layers=num_encoder_layers, \n",
    "                                                num_decoder_layers=num_decoder_layers, \n",
    "                                                dim_feedforward=dim_feedforward, \n",
    "                                                dropout=dropout,\n",
    "                                                batch_first=False)\n",
    "        \n",
    "        self.head = nn.Linear(dim, out_vs)\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        #print(f'{x.shape=} {tgt.shape=}')\n",
    "        #print(f'{x=} {tgt=}')\n",
    "        src_key_padding_mask = (x == 0).T\n",
    "        tgt_key_padding_mask = (tgt == 0).T\n",
    "        x = self.embeddings(x) * math.sqrt(self.dim)\n",
    "        x = self.pos_embs(x)\n",
    "        #print(f'{x.shape=}')\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.shape[0]).to(x.device)\n",
    "        tgt = self.out_embs(tgt) * math.sqrt(self.dim)\n",
    "        tgt = self.pos_embs(tgt)\n",
    "        #print(f'{tgt.shape=}')\n",
    "\n",
    "        out = self.transformer(x, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61bf568-d030-4033-9739-4268cccca2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.Tensor([phone_counts[p] if p in phone_counts else 0 for p in tokenizer.p_id2p])\n",
    "weights/=weights.sum()\n",
    "weights[0] = 0\n",
    "weights[1] = 1\n",
    "weights[2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f47fee-0481-4dbd-b98f-e231d6b3862e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.0000e+00, 1.0000e+00, 1.9700e-02, 3.1290e-02, 3.8468e-02,\n",
       "        2.7842e-03, 3.8907e-02, 1.7350e-01, 2.3888e-02, 1.1140e-01, 1.3771e-02,\n",
       "        4.6623e-02, 4.0744e-02, 1.1991e-02, 3.1570e-02, 3.1225e-02, 8.0657e-03,\n",
       "        2.9189e-02, 7.8236e-03, 2.9307e-02, 1.8865e-02, 1.0952e-02, 1.3442e-02,\n",
       "        1.2296e-02, 1.8934e-02, 3.7075e-02, 1.3714e-02, 2.0371e-02, 1.2007e-02,\n",
       "        2.3892e-02, 8.0324e-03, 1.0418e-02, 3.2077e-02, 5.0375e-03, 2.4680e-02,\n",
       "        7.9802e-03, 1.3897e-02, 4.0033e-03, 4.6003e-03, 9.7202e-03, 1.4531e-03,\n",
       "        4.9651e-03, 3.4787e-04, 9.9579e-04])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b49be729-fec4-46ad-8135-4c336e6660c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitBase(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4, target='phones'):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    def forward(self, x, tgt):\n",
    "        # if not isinstance(x, torch.Tensor):\n",
    "        #     if isinstance(x, list) and len(x) == 2 :\n",
    "        #         x = x[0]\n",
    "        #     else:\n",
    "        #         print(f'{len(x)} {x[0].shape=} {x[1].shape=}')\n",
    "        #         raise RuntimeError()\n",
    "        return self.model(x, tgt)\n",
    "    \n",
    "    def compute_loss(self, batch, batch_idx):\n",
    "        x, tgt = batch['chars'], batch[self.target]\n",
    "        #print(f'{x.shape=} {y.shape=}')\n",
    "        logits = self(x, tgt)\n",
    "        logits = logits[:-1].view(-1, logits.shape[-1])\n",
    "        \n",
    "        y = tgt[1:].view(-1)\n",
    "        #print(f'{logits.shape=}')\n",
    "        loss = self.criterion(logits, y)\n",
    "        #print(logits, y, loss)\n",
    "        return loss, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Logging to TensorBoard by default\n",
    "        loss, logits = self.compute_loss(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logits = self.compute_loss(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss,  prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        lr_scheduler = lr_scheduler_config = {           \n",
    "            \"scheduler\": torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95),\n",
    "            \"interval\": \"epoch\"}\n",
    "        return {'optimizer': optimizer,\n",
    "                'lr_scheduler': lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89dc341e-3b85-4d76-a817-92d807f33a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 274146\n",
      "Total uniq words 75238\n",
      "Total words 30523\n",
      "Total uniq words 13878\n"
     ]
    }
   ],
   "source": [
    "train_ds = UniqTranscriptorDataSet(train_sents, tokenizer)\n",
    "test_ds = UniqTranscriptorDataSet(test_sents, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8e98ddb-3d19-4dee-8fd0-54b8855aeddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "btz=256\n",
    "train_dl = make_sorted_dataloader(train_ds, batch_size=btz)\n",
    "test_dl = make_sorted_dataloader(test_ds, batch_size=btz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19420403-6afd-4869-a2b4-1be3703633b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 44)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_num_chars(), tokenizer.get_num_phones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad53e07-d5c3-4f65-81e3-705cd93192f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = LitBase(TranscriptTransformer(tokenizer.get_num_chars()+1, tokenizer.get_num_allos()+1), lr=1e-4, target='allos')\n",
    "#tt = tt.load_from_checkpoint(model=TranscriptTransformer(tokenizer.get_num_chars()+1, tokenizer.get_num_phones()+1), checkpoint_path='models/tt2/lightning_logs/version_0/checkpoints/epoch=99-step=25799.ckpt')\n",
    "trainer = pl.Trainer(gpus=1, \n",
    "                     auto_lr_find=True, \n",
    "                     max_epochs=100, \n",
    "                     log_every_n_steps=300, \n",
    "                     default_root_dir=\"models/tt_allo\",)\n",
    "                    #overfit_batches=10)\n",
    "#trainer.tune(tt, train_dl, test_dl) # fit(dnn, train_dl)\n",
    "print(f\"best lr is {tt.lr}\")\n",
    "trainer.fit(tt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36e5f9cc-6a4c-4876-978d-6e9638a366ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchmetrics.functional.text.wer(['i am ready'], ['i am reading 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e5fb5a3-227f-414f-a9bd-01d67c2f4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2 = tt.load_from_checkpoint(model=TranscriptTransformer(tokenizer.get_num_chars()+1, tokenizer.get_num_allos()+1), target='allos',\n",
    "                             checkpoint_path='models/tt_allo/lightning_logs/version_2/checkpoints/epoch=99-step=29399.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "637e64dd-7185-443f-bc51-9ec9544e114a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'же',\n",
       " 'chars': tensor([ 1, 11,  9,  2]),\n",
       " 'allos': tensor([ 1, 40, 16,  2]),\n",
       " 'allos_str': ['<SOW>', 'zh', 'y4', '<EOW>']}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(word, tokenizer, model, max_len=10):\n",
    "    x = tokenizer.tokenize_chars(word).view(-1, 1)\n",
    "    if model.target == 'phones':\n",
    "        y = torch.LongTensor([tokenizer.get_phone_sos()])\n",
    "    elif model.target == 'allos':\n",
    "        y = torch.LongTensor([tokenizer.get_allo_sos()])\n",
    "    for i in range(max_len):\n",
    "        pred = model(x, y.view(-1, 1))\n",
    "        next_y = pred[-1, 0, 1:].argmax()+1\n",
    "        y = torch.cat((y, torch.LongTensor([next_y])))\n",
    "        if next_y == tokenizer.get_phone_eos():\n",
    "            break\n",
    "    return {'original': word,\n",
    "            'chars': x.view(-1),\n",
    "            model.target: y,\n",
    "            f'{model.target}_str': tokenizer.convert_to_str(y, model.target)}\n",
    "generate('же', tokenizer, tt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d87ecec3-7647-4643-b9a7-25e7b0c47275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'кажется,',\n",
       " 'ref_ids': tensor([ 1, 41,  8, 40, 16, 21, 23,  2]),\n",
       " 'ref': '<SOW> k a0 zh y4 c a4 <EOW>',\n",
       " 'hyp_ids': tensor([ 1, 41,  8, 40, 16, 21, 23,  2]),\n",
       " 'hyp': '<SOW> k a0 zh y4 c a4 <EOW>',\n",
       " 'Error Rate': tensor(0.)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_and_cmp(item, tokenizer, model, max_len=10):\n",
    "    gen = generate(item['original'], tokenizer, model, max_len=max_len)\n",
    "    ref = ' '.join(tokenizer.convert_to_str(item[model.target], model.target))\n",
    "    hyp = ' '.join(gen[f'{model.target}_str'])\n",
    "    cer = torchmetrics.functional.text.wer(hyp, ref)\n",
    "    return {'original': item['original'],\n",
    "            'ref_ids': item[model.target],\n",
    "            'ref': ref,\n",
    "            'hyp_ids': gen[model.target],\n",
    "            'hyp': hyp,\n",
    "            'Error Rate': cer}\n",
    "\n",
    "generate_and_cmp(test_ds[9], tokenizer, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65cded73-73e9-424c-802b-fb0a552f4f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4d2d6df1b345df88c2c690c22de2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate is 0.08138975501060486\n"
     ]
    }
   ],
   "source": [
    "total_wer=0\n",
    "total_count=0\n",
    "for i in tqdm(range(len(test_ds))[:1000]):\n",
    "    item = test_ds[i]\n",
    "    report = generate_and_cmp(item, tokenizer, tt)\n",
    "    total_wer += report['Error Rate']*len(report['ref_ids'])\n",
    "    total_count += len(report['ref_ids'])\n",
    "print(f\"Phone error rate is {total_wer/total_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9838d5-d8ac-47d5-93eb-42ff1ce4fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171bae0-f31e-426f-bb84-2696ee32494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn.heatmap(logits.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a895df4-63a2-4f4e-b884-aa210f9dbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base = get_base('data/lab2/test_submit.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "589cc696-4373-4b13-a491-10d930dcc411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'original': 'Реально', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'начать', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'день', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'с', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'улыбки -', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'это', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'проснуться', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'от', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'звонка', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'будильника', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'в', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'одну', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'минуту', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'первого', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'ночи,', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'широко', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'улыбнуться', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'и', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'спать', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'дальше.', 'ph_trans': [], 'allo_trans': []}],\n",
       " [{'original': 'Согласитесь,', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'было', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'бы', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'гораздо', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'лучше,', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'если', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'бы', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'во', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'всех', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'фильмах,', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'в', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'которых', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'играет', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'Петров,', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'Петрова', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'бы', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'играл', 'ph_trans': [], 'allo_trans': []},\n",
       "  {'original': 'Безруков.', 'ph_trans': [], 'allo_trans': []}]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_base[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6959930a-1614-4844-89c7-71568042055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d952115dae4ac29fbfb4d227d8f46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'original': 'Реально', 'chars': tensor([ 1, 21,  9,  4, 16, 33, 18, 19,  2]), 'allos': tensor([ 1, 46, 33,  8, 31, 29, 23,  2]), 'allos_str': ['<SOW>', \"r'\", 'i1', 'a0', \"l'\", 'n', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'начать', 'chars': tensor([ 1, 18,  4, 28,  4, 23, 33,  2]), 'allos': tensor([ 1, 29, 10, 35,  8, 24,  2]), 'allos_str': ['<SOW>', 'n', 'a1', 'ch', 'a0', \"t'\", '<EOW>'], 'allo_trans': []}, {'original': 'день', 'chars': tensor([ 1,  8,  9, 18, 33,  2]), 'allos': tensor([ 1, 27, 20, 38,  2]), 'allos_str': ['<SOW>', \"d'\", 'e0', \"n'\", '<EOW>'], 'allo_trans': []}, {'original': 'с', 'chars': tensor([ 1, 22,  2]), 'allos': tensor([ 1, 20, 17,  2]), 'allos_str': ['<SOW>', 'e0', 's', '<EOW>'], 'allo_trans': []}, {'original': 'улыбки -', 'chars': tensor([ 1, 24, 16, 32,  5, 15, 13,  3, 41,  2]), 'allos': tensor([ 1,  5, 28, 37, 43, 52, 32,  2]), 'allos_str': ['<SOW>', 'u1', 'l', 'y0', 'p', \"k'\", 'i4', '<EOW>'], 'allo_trans': []}, {'original': 'это', 'chars': tensor([ 1, 34, 23, 19,  2]), 'allos': tensor([ 1, 20, 13, 23,  2]), 'allos_str': ['<SOW>', 'e0', 't', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'проснуться', 'chars': tensor([ 1, 20, 21, 19, 22, 18, 24, 23, 33, 22, 36,  2]), 'allos': tensor([ 1, 43,  4, 18, 17, 29, 34, 21, 23,  2]), 'allos_str': ['<SOW>', 'p', 'r', 'a2', 's', 'n', 'u0', 'c', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'от', 'chars': tensor([ 1, 19, 23,  2]), 'allos': tensor([ 1, 14, 13,  2]), 'allos_str': ['<SOW>', 'o0', 't', '<EOW>'], 'allo_trans': []}, {'original': 'звонка', 'chars': tensor([ 1, 12,  6, 19, 18, 15,  4,  2]), 'allos': tensor([ 1, 26, 22, 10, 29, 41,  8,  2]), 'allos_str': ['<SOW>', 'z', 'v', 'a1', 'n', 'k', 'a0', '<EOW>'], 'allo_trans': []}, {'original': 'будильника', 'chars': tensor([ 1,  5, 24,  8, 13, 16, 33, 18, 13, 15,  4,  2]), 'allos': tensor([ 1, 15,  5, 27, 11, 31, 38, 32, 41, 23,  2]), 'allos_str': ['<SOW>', 'b', 'u1', \"d'\", 'i0', \"l'\", \"n'\", 'i4', 'k', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'в', 'chars': tensor([1, 6, 2]), 'allos': tensor([ 1, 50,  2]), 'allos_str': ['<SOW>', 'f', '<EOW>'], 'allo_trans': []}, {'original': 'одну', 'chars': tensor([ 1, 19,  8, 18, 24,  2]), 'allos': tensor([ 1, 10,  3, 29, 34,  2]), 'allos_str': ['<SOW>', 'a1', 'd', 'n', 'u0', '<EOW>'], 'allo_trans': []}, {'original': 'минуту', 'chars': tensor([ 1, 17, 13, 18, 24, 23, 24,  2]), 'allos': tensor([ 1, 44, 33, 29, 34, 13, 45,  2]), 'allos_str': ['<SOW>', \"m'\", 'i1', 'n', 'u0', 't', 'u4', '<EOW>'], 'allo_trans': []}, {'original': 'первого', 'chars': tensor([ 1, 20,  9, 21,  6, 19,  7, 19,  2]), 'allos': tensor([ 1, 42, 20,  4, 22, 23, 22, 23,  2]), 'allos_str': ['<SOW>', \"p'\", 'e0', 'r', 'v', 'a4', 'v', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'ночи,', 'chars': tensor([ 1, 18, 19, 28, 13, 37,  2]), 'allos': tensor([ 1, 29, 14, 35, 32,  2]), 'allos_str': ['<SOW>', 'n', 'o0', 'ch', 'i4', '<EOW>'], 'allo_trans': []}, {'original': 'широко', 'chars': tensor([ 1, 29, 13, 21, 19, 15, 19,  2]), 'allos': tensor([ 1, 12, 48,  4, 10, 41, 14,  2]), 'allos_str': ['<SOW>', 'sh', 'y1', 'r', 'a1', 'k', 'o0', '<EOW>'], 'allo_trans': []}, {'original': 'улыбнуться', 'chars': tensor([ 1, 24, 16, 32,  5, 18, 24, 23, 33, 22, 36,  2]), 'allos': tensor([ 1,  5, 28, 48, 15, 29, 34, 21, 23,  2]), 'allos_str': ['<SOW>', 'u1', 'l', 'y1', 'b', 'n', 'u0', 'c', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'и', 'chars': tensor([ 1, 13,  2]), 'allos': tensor([ 1, 33,  2]), 'allos_str': ['<SOW>', 'i1', '<EOW>'], 'allo_trans': []}, {'original': 'спать', 'chars': tensor([ 1, 22, 20,  4, 23, 33,  2]), 'allos': tensor([ 1, 17, 43,  8, 24,  2]), 'allos_str': ['<SOW>', 's', 'p', 'a0', \"t'\", '<EOW>'], 'allo_trans': []}, {'original': 'дальше.', 'chars': tensor([ 1,  8,  4, 16, 33, 29,  9, 38,  2]), 'allos': tensor([ 1,  3,  8, 31, 12, 16,  2]), 'allos_str': ['<SOW>', 'd', 'a0', \"l'\", 'sh', 'y4', '<EOW>'], 'allo_trans': []}], [{'original': 'Согласитесь,', 'chars': tensor([ 1, 22, 19,  7, 16,  4, 22, 13, 23,  9, 22, 33, 37,  2]), 'allos': tensor([ 1, 17, 18, 25, 28, 10, 30, 11, 24, 32, 30,  2]), 'allos_str': ['<SOW>', 's', 'a2', 'g', 'l', 'a1', \"s'\", 'i0', \"t'\", 'i4', \"s'\", '<EOW>'], 'allo_trans': []}, {'original': 'было', 'chars': tensor([ 1,  5, 32, 16, 19,  2]), 'allos': tensor([ 1, 15, 16, 28, 23,  2]), 'allos_str': ['<SOW>', 'b', 'y4', 'l', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'бы', 'chars': tensor([ 1,  5, 32,  2]), 'allos': tensor([ 1, 15, 16,  2]), 'allos_str': ['<SOW>', 'b', 'y4', '<EOW>'], 'allo_trans': []}, {'original': 'гораздо', 'chars': tensor([ 1,  7, 19, 21,  4, 12,  8, 19,  2]), 'allos': tensor([ 1, 25, 18,  4, 10, 26,  3, 14,  2]), 'allos_str': ['<SOW>', 'g', 'a2', 'r', 'a1', 'z', 'd', 'o0', '<EOW>'], 'allo_trans': []}, {'original': 'лучше,', 'chars': tensor([ 1, 16, 24, 28, 29,  9, 37,  2]), 'allos': tensor([ 1, 28, 34, 35, 12, 16,  2]), 'allos_str': ['<SOW>', 'l', 'u0', 'ch', 'sh', 'y4', '<EOW>'], 'allo_trans': []}, {'original': 'если', 'chars': tensor([ 1,  9, 22, 16, 13,  2]), 'allos': tensor([ 1,  7, 20, 17, 31, 32,  2]), 'allos_str': ['<SOW>', 'j', 'e0', 's', \"l'\", 'i4', '<EOW>'], 'allo_trans': []}, {'original': 'бы', 'chars': tensor([ 1,  5, 32,  2]), 'allos': tensor([ 1, 15, 16,  2]), 'allos_str': ['<SOW>', 'b', 'y4', '<EOW>'], 'allo_trans': []}, {'original': 'во', 'chars': tensor([ 1,  6, 19,  2]), 'allos': tensor([ 1, 22, 14,  2]), 'allos_str': ['<SOW>', 'v', 'o0', '<EOW>'], 'allo_trans': []}, {'original': 'всех', 'chars': tensor([ 1,  6, 22,  9, 26,  2]), 'allos': tensor([ 1, 50, 30, 20, 39,  2]), 'allos_str': ['<SOW>', 'f', \"s'\", 'e0', 'h', '<EOW>'], 'allo_trans': []}, {'original': 'фильмах,', 'chars': tensor([ 1, 25, 13, 16, 33, 17,  4, 26, 37,  2]), 'allos': tensor([ 1, 55, 11, 31,  9, 23, 39,  2]), 'allos_str': ['<SOW>', \"f'\", 'i0', \"l'\", 'm', 'a4', 'h', '<EOW>'], 'allo_trans': []}, {'original': 'в', 'chars': tensor([1, 6, 2]), 'allos': tensor([ 1, 22, 48, 22,  2]), 'allos_str': ['<SOW>', 'v', 'y1', 'v', '<EOW>'], 'allo_trans': []}, {'original': 'которых', 'chars': tensor([ 1, 15, 19, 23, 19, 21, 32, 26,  2]), 'allos': tensor([ 1, 41, 10, 13, 14,  4, 16, 39,  2]), 'allos_str': ['<SOW>', 'k', 'a1', 't', 'o0', 'r', 'y4', 'h', '<EOW>'], 'allo_trans': []}, {'original': 'играет', 'chars': tensor([ 1, 13,  7, 21,  4,  9, 23,  2]), 'allos': tensor([ 1, 33, 25,  4,  8,  7, 32, 13,  2]), 'allos_str': ['<SOW>', 'i1', 'g', 'r', 'a0', 'j', 'i4', 't', '<EOW>'], 'allo_trans': []}, {'original': 'Петров,', 'chars': tensor([ 1, 20,  9, 23, 21, 19,  6, 37,  2]), 'allos': tensor([ 1, 42, 33, 13,  4, 14, 50,  2]), 'allos_str': ['<SOW>', \"p'\", 'i1', 't', 'r', 'o0', 'f', '<EOW>'], 'allo_trans': []}, {'original': 'Петрова', 'chars': tensor([ 1, 20,  9, 23, 21, 19,  6,  4,  2]), 'allos': tensor([ 1, 42, 33, 13,  4, 14, 22, 23,  2]), 'allos_str': ['<SOW>', \"p'\", 'i1', 't', 'r', 'o0', 'v', 'a4', '<EOW>'], 'allo_trans': []}, {'original': 'бы', 'chars': tensor([ 1,  5, 32,  2]), 'allos': tensor([ 1, 15, 16,  2]), 'allos_str': ['<SOW>', 'b', 'y4', '<EOW>'], 'allo_trans': []}, {'original': 'играл', 'chars': tensor([ 1, 13,  7, 21,  4, 16,  2]), 'allos': tensor([ 1, 33, 25,  4,  8, 28,  2]), 'allos_str': ['<SOW>', 'i1', 'g', 'r', 'a0', 'l', '<EOW>'], 'allo_trans': []}, {'original': 'Безруков.', 'chars': tensor([ 1,  5,  9, 12, 21, 24, 15, 19,  6, 38,  2]), 'allos': tensor([ 1, 49, 33, 26,  4, 34, 41, 23, 50,  2]), 'allos_str': ['<SOW>', \"b'\", 'i1', 'z', 'r', 'u0', 'k', 'a4', 'f', '<EOW>'], 'allo_trans': []}]]\n"
     ]
    }
   ],
   "source": [
    "test_base_predicted = []\n",
    "for seq in tqdm(test_base):\n",
    "    seq_predicted = []\n",
    "    for item in seq:\n",
    "        #print(item)\n",
    "        gen = generate(item['original'].lower(), tokenizer, tt, max_len=15)\n",
    "        gen['original'] = item['original']\n",
    "        gen['allo_trans'] = item['allo_trans']\n",
    "        \n",
    "        #gen = generate_and_cmp(item, tokenizer, tt, max_len=15)\n",
    "        seq_predicted.append(gen)\n",
    "    test_base_predicted.append(seq_predicted)\n",
    "print(test_base_predicted[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d0437ec5-a230-45d7-a87c-36adc5cbde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "\n",
    "import pprint\n",
    "def to_json(base, fname):\n",
    "    root_json = []\n",
    "    for sen in base:\n",
    "        seq_json = []\n",
    "        for item in sen:\n",
    "            word = {'content': item['original'],\n",
    "                   'allophones': item['allos_str'][1:-1]}\n",
    "            seq_json.append(word)\n",
    "        root_json.append({'words': seq_json})\n",
    "    #root_json = root_json[:2]\n",
    "    with open(fname, 'w', encoding='utf-8') as outfile:\n",
    "        out_str = json.dump(root_json, outfile, ensure_ascii=False, indent=4)\n",
    "    #return {'root': root_json}\n",
    "    return out_str\n",
    "\n",
    "to_json(test_base_predicted, 'data/lab2/submit_mitrofanov2.5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4db64a1-ddf5-4c36-9736-291dde70c495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://nid-artifactory.ad.speechpro.com/artifactory/api/pypi/pypi/simple, https://nid-artifactory.ad.speechpro.com/artifactory/api/pypi/asr3-pip-local/simple\n",
      "Requirement already satisfied: simplejson in /mnt/hdd/mitrofanov-aa/project/study/anaconda/envs/sr/lib/python3.8/site-packages (3.17.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7324196-3608-40e3-91ae-b8425229e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head 'data/lab2/submit_mitrofanov2.5.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab712c9c-673a-4345-9f56-4d7f3a52d9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
