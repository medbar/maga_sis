{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9933fce4-d71d-4117-94c4-3122e1c27377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5c3ff9-1b1d-485f-84f7-c6f60588c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import make_sorted_dataloader, collate_with_paddings\n",
    "from utils.chunk_dataset import ChunkDataSet\n",
    "from utils.zip_dataset import ZippedDataSet\n",
    "from utils.chunk_augmented_dataset import ChunkAugDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05998234-8329-45c7-a698-87af6a350d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2Model, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6267976e-b18d-40f2-8f60-94fdf39cef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41])\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "#model = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "class Wav2Vec2ED(nn.Module): \n",
    "    def __init__(self,  sample_rate=16000, n_classes=41):\n",
    "        super().__init__()\n",
    "        #self.ms = torchaudio.transforms.MelSpectrogram(sample_rate)\n",
    "        #self.bn0 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        #self.cnn1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1)\n",
    "        #self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "        #self.cnn3 = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, padding=1)\n",
    "        #self.bn3 = nn.BatchNorm2d(3)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        \n",
    "        self.features = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        for param in  self.features.encoder.pos_conv_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "        # use it as features\n",
    "#         for param in self.features.parameters():\n",
    "#             param.requires_grad = False\n",
    "        #self.mp = torch.nn.MaxPool1d(3)\n",
    "        self.lin1 = nn.Linear(768, 111)\n",
    "              \n",
    "        self.lin2 = nn.Linear(111, n_classes)\n",
    "        self.freeze = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.ms(x)\n",
    "        #x = self.bn0(x)\n",
    "        #x = F.relu(self.cnn1(x))\n",
    "        #x = self.dropout(F.relu(self.bn1(self.cnn1(x))))\n",
    "        #x = self.dropout(F.relu(self.bn2(self.cnn2(x))))\n",
    "        #x = F.relu(self.bn3(self.cnn3(x)))\n",
    "        if self.freeze:\n",
    "            with torch.no_grad():\n",
    "                x = self.features(x).last_hidden_state\n",
    "        else:\n",
    "            x = self.features(x).last_hidden_state\n",
    "\n",
    "#         x = x.view(x.shape[0], -1)\n",
    "        #print(x.shape)\n",
    "        # x = F.relu(x.transpose(1, 2))\n",
    "        # x = self.mp(x)\n",
    "        # x = F.relu(self.conv1d1(x))\n",
    "        # x = F.relu(self.conv1d2(x))\n",
    "        # #print(x.shape)\n",
    "        # x = x.flatten(1)\n",
    "        #x = torch.mean(x, dim=1)\n",
    "        #x = x.flatten(1)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        x = F.relu(self.lin1(x[:,-1,:]))\n",
    "        #x = F.relu(self.lin2(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "    def inference(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "out = Wav2Vec2ED()(torch.ones(1, 32000))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d9bfead-35c0-420a-bfc1-4ecee23b64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = \n",
    "# chunk_sz=6\n",
    "# train_ds = ZippedDataSet(f'data/Annotations_gold_standard/English/train_chunk{chunk_sz}s_msl0.zip', ChunkDataSet('./data/Annotations_gold_standard/English/train/', chunk_size_s=chunk_sz), rewrite=False, min_sample_len=0)\n",
    "# test_ds = ZippedDataSet(f'data/Annotations_gold_standard/English/test_chunk{chunk_sz}s_msl0.zip', ChunkDataSet('./data/Annotations_gold_standard/English/test', chunk_size_s=chunk_sz), rewrite=False, min_sample_len=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da6e0827-6dd3-49fb-ae4f-33ddd5e09cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = torchaudio.load('data/Annotations_gold_standard/English/train/027_2016-04-06_Nottingham/expert.audio[48000]_clean.wav')\n",
    "wav = torchaudio.functional.resample(wav, sr, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd17fea-6e6f-4733-8408-ef005862253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ED().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc33626-01c1-496c-852e-0cfe29e5cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(wav[:, :5000].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de8d3848-bb65-4fc8-9e7c-8459cec4cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 41])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5cc0a0f-976b-4b65-abf7-a6ec248f02d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0621,  0.0606, -0.0752,  0.0157, -0.0388, -0.0562,  0.1037,  0.1108,\n",
       "         -0.1186,  0.1355, -0.0447,  0.1238,  0.0658, -0.0163, -0.0455,  0.1989,\n",
       "          0.0986, -0.0378,  0.0057,  0.0622,  0.0840,  0.0244,  0.0667,  0.0345,\n",
       "         -0.0471,  0.1396, -0.0814,  0.1465, -0.0858,  0.0294, -0.0677,  0.0872,\n",
       "          0.1620, -0.0876,  0.0252, -0.0770, -0.0569,  0.0558,  0.0677, -0.1311,\n",
       "          0.1163]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a36c0-7157-44d4-96f8-f51b4a102d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
